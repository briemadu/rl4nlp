---
title: Home
layout: default
nav_order: 1
---

### Universität Potsdam, MSc CogSys, Summer 2020
# Seminar: Reinforcement Learning for Natural Language Processing

|-------------:|---------------------------|
| :woman_teacher:   | Brielen Madureira   |
| :date: | Online |
| :hourglass: | 20.04.2020 - 20.07.2020 | 
| :computer:| [Moodle page, course RL4NLP id 23952](<https://moodle2.uni-potsdam.de/course/view.php?id=23952>) |
| :e-mail:  |  madureiralasota @ uni-potsdam . de | 

## Description
Reinforcement Learning (RL) is a machine learning paradigm that is becoming more popular for Natural Language Processing (NLP). In this seminar, we will cover recent literature that apply RL to different NLP tasks, discussing how NLP problems can be modeled with RL concepts and comparing the results to other traditional or common approaches. 

## Goals
An important skill that all NLP researchers and practitioners must have is being able to critically read papers. Just because a paper has been presented at a conference or published on a journal, it does not necessarily mean that it is flawless. We, as responsible readers, must be able to understand the methodology, evaluate the approach and build upon the work of our colleagues to improve our field. Besides becoming up-to-date with the applications of RL in NLP, the purpose of this course is also helping students getting used to constructively assessing and discussing scientific literature.

*Disclaimer*: due to the format of the course and the limited time, this course will not teach Reinforcement Learning *per se*. The main abstract concepts will come up in the discussions, but if you wish to learn its mathematical foundations or algorithms in detail, take a look, e.g. at [David Silver's video-lectures](https://www.davidsilver.uk/teaching/), [Emma Brunskill's course](https://web.stanford.edu/class/cs234/index.html) and the [reference book by Sutton & Barto](https://opac.ub.uni-potsdam.de/DB=1/SET=2/TTL=1/SHW?FRST=1). 

## Format
We will work as "reviewers for one week". Each week, one participant pretends to be an NLP conference reviewer and writes a text in the form of a blog post, following the guidelines below, about one of the papers in the bibliography. This will be posted on Moodle together with three topics you propose for group discussion in the forum. Throughout the week, everyone takes part on the debate at least once. Then you and I meet virtually for a short spontaneous talk about your impressions. In the end of the semester, you will work on an essay as part of the final grade.

:warning: If you are considering taking this seminar, please write me an e-mail or register in the Moodle course, to help me plan our schedule. You can also tell me if you have any preference regarding the papers in the bibliography, or suggest a paper that is note cited there yet. :warning: 

## Background 
Everyone is welcome to take this class, but you will profit more if you are already familiar with the main NLP tasks and methods. Watching the online courses I mention above is also a great help. If you are in doubt, contact me :)

## Grading
Grading will be based on your participation on the discussions, your paper review and the final project.

## Formalities
Step-by-step guidelines (these are current ideas, they may be changed until the beginning of the course).

During the first week of the course, I will publish the schedule with papers and reviewers here. The cycle will be as follows:

Every participant is a debater except during the week when she/he is a reviewer. Each one of you will be the reviewer once, working on one paper of your choice.

**Reviewer**
1) Your scheduled Monday before 11:59 a.m.: send me your review and 3 discussion topics.
2) Following Friday at 3:00 p.m.: we meet online; come prepared to first hold a 15-minute talk where you tell me your impressions on the group discussion.


<details>
  <summary>Guidelines</summary>
  
  Your post should be written on markdown style.
  
</details>

**Debater**
1) Monday before 11:59 a.m.: skim through the current week's paper (read at least the abstract, introduction, conclusion) and send me a two-paragraph text.
2) Anytime before Friday at 11:59 a.m.: post at least one contribution to the discussion on Moodle. This should be pertinent to the development of the discussion so far and consist of at least 200 words. 


Reviewer for a week: each participant chooses a paper from the bibliography. You pretend you are a reviewer in a big conference and evaulate the paper. Then, you write a text in the form of a blog post discussing the accomplishments and limitations of the paper, together with a short review on other approaches for solving the same task and suggestion for future work. This should be done aiming at a public that is not familiar with the field. Provide 3 topics for discussion that will be carried out by your colleagues during the week. Then we meet virtually for a 10-15 minute private talk, when you tell me about your impressions on the discussion.

## Bibliography

<details>
  <summary>A non-exhaustive list of RL+NLP papers </summary>
  
  # Coreference resolution
  * CLARK, Kevin; MANNING, Christopher D. Deep Reinforcement Learning for Mention-Ranking Coreference Models. In: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 2016. S. 2256-2262. [link](https://www.aclweb.org/anthology/D16-1245/)
  
  # Dependenca parsing 
  * LÊ, Minh; FOKKENS, Antske. Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing. In: Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. 2017. S. 677-687. [link](https://www.aclweb.org/anthology/E17-1064/)
  * ZHANG, Lidan; CHAN, Kwok Ping. Dependency parsing with energy-based reinforcement learning. In: Proceedings of the 11th International Conference on Parsing Technologies. Association for Computational Linguistics, 2009. S. 234-237. [link](https://www.aclweb.org/anthology/W09-3838/)
  
  # Dialog
  * ENGLISH, Michael S.; HEEMAN, Peter A. Learning mixed initiative dialog strategies by using reinforcement learning on both conversants. In: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2005. S. 1011-1018. [link](https://dl.acm.org/doi/10.3115/1220575.1220702)
  * FANG, Meng; LI, Yuan; COHN, Trevor. Learning how to Active Learn: A Deep Reinforcement Learning Approach. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017. S. 595-605. [link](https://www.aclweb.org/anthology/D17-1063/)
  * PAEK, Tim. Reinforcement learning for spoken dialogue systems: Comparing strengths and weaknesses for practical deployment. In: Proc. Dialog-on-Dialog Workshop, Interspeech. 2006. [link](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.4229&rep=rep1&type=pdf)
  * PAPANGELIS, Alexandros. A Comparative Study of Reinforcement Learning Techniques on Dialogue Management. In: Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics. 2012. S. 22-31. [link](https://www.aclweb.org/anthology/E12-3003/)
  * SINGH, Satinder P., et al. Reinforcement learning for spoken dialogue systems. In: Advances in Neural Information Processing Systems. 2000. S. 956-962. [link](http://papers.nips.cc/paper/1775-reinforcement-learning-for-spoken-dialogue-systems.pdf)
  
  # Grammatical error correction
  * SAKAGUCHI, Keisuke; POST, Matt; VAN DURME, Benjamin. Grammatical Error Correction with Neural Reinforcement Learning. In: Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2017. S. 366-372. [link](https://www.aclweb.org/anthology/I17-2062/)
  
  # Human-robot interaction
  * CRUZ, Francisco, et al. Interactive reinforcement learning through speech guidance in a domestic scenario. In: 2015 International Joint Conference on Neural Networks (IJCNN). IEEE, 2015. S. 1-8. [link](https://ieeexplore.ieee.org/abstract/document/7280477)
  
  # Information Extraction
  * NARASIMHAN, Karthik; YALA, Adam; BARZILAY, Regina. Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning. In: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 2016. S. 2355-2365. [link](https://www.aclweb.org/anthology/D16-1261/)
  
  # Instructions
  * BRANAVAN, Satchuthananthavale RK, et al. Reinforcement learning for mapping instructions to actions. In: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1. Association for Computational Linguistics, 2009. S. 82-90. [link](https://dl.acm.org/doi/10.5555/1687878.1687892)
  
  # Knowledge graph reasoning
  * XIONG, Wenhan; HOANG, Thien; WANG, William Yang. DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017. S. 564-573. [link](https://www.aclweb.org/anthology/D17-1060/)
  
  # Language model
  * RANZATO, Marc'Aurelio, et al. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015. [link](https://arxiv.org/abs/1511.06732)
  
  # Machine Translation
  * GRISSOM II, Alvin, et al. Don’t until the final verb wait: Reinforcement learning for simultaneous machine translation. In: Proceedings of the 2014 Conference on empirical methods in natural language processing (EMNLP). 2014. S. 1342-1352. [link](https://www.aclweb.org/anthology/D14-1140/)
  
  # Math word problem
  * HUANG, Danqing, et al. Neural math word problem solver with reinforcement learning. In: Proceedings of the 27th International Conference on Computational Linguistics. 2018. S. 213-223. [link](https://www.aclweb.org/anthology/C18-1018/)
  
  # Relation Extraction
  * ZENG, Xiangrong, et al. Large scaled relation extraction with reinforcement learning. In: Thirty-Second AAAI Conference on Artificial Intelligence. 2018. [link](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16257)
  
  # Taxonomy induction
  * MAO, Yuning, et al. End-to-End Reinforcement Learning for Automatic Taxonomy Induction. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018. S. 2462-2472. [link](https://www.aclweb.org/anthology/P18-1229/)
  
  # Text-based games
  
  # Text classification
  * ZHANG, Tianyang; HUANG, Minlie; ZHAO, Li. Learning structured representation for text classification via reinforcement learning. In: Thirty-Second AAAI Conference on Artificial Intelligence. 2018. [link](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16537)

  # Text summarization
  * LEE, Gyoung Ho; LEE, Kong Joo. Automatic text summarization using reinforcement learning with embedding features. In: Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2017. S. 193-197. [link](https://www.aclweb.org/anthology/I17-2033/)
  * PAULUS, Romain; XIONG, Caiming; SOCHER, Richard. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.[link](https://arxiv.org/abs/1705.04304)
  
  # Video captioning
  * WANG, Xin, et al. Video captioning via hierarchical reinforcement learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. S. 4213-4222. [link](http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Video_Captioning_via_CVPR_2018_paper.html)

  
</details>

## Contact
Feel free to write me if you have any question or suggestion.
